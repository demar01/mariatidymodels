---
title: "yardstick: assessing model performance "
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{yardstick}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# EDA to select a model

```{r, message=FALSE}
library(tidymodels)
library(usemodels)
library(palmerpenguins)
library(broom)
library(mariatidymodels)
```

Take the penguins dataset; this dataset has variables (bill_length,
bill_depth, flipper_length etc) for 3 species of penguins. These
variables really are what are used to classify penguins into species. In
the \`penguins\` dataset, there is also information about sex. Let's
plot that:

```{r}
penguins %>%
    filter(!is.na(sex)) %>%
    ggplot(aes(bill_length_mm, bill_depth_mm, color = sex, size = body_mass_g)) +
    geom_point(alpha = 0.5) +
    facet_wrap(~species)
```

It looks like females then to have smaller bill length and depth. And it
seems like body weigh is more specific for the specie ( i.e. Gentoo
penguins look heavier). We could build a classification model that
predict whether a penguin is female or male using the other from the
other 5 body characteristics in the dataset. Then we can calculate
several assessment measurements with *yardstick.*

```{r}
penguins_df <- penguins %>%
  filter(!is.na(sex)) %>%
  dplyr::select(-year, -island)
```

# Resampling the data

```{r}
set.seed(123)
penguin_split <- initial_split(penguins_df, strata = sex)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)

k_folds <- bootstraps(penguin_train)
```

# Building model with usemodels

`usemodels` gives you such a great boilerplate to customise models. For
instance, we can use `r use_kknn(sex ~ ., data = penguin_train)` and
customize the output model.

```{r}
use_kknn(sex ~ ., data = penguin_train)
```

I want to be able to set metrics of the model performance, and be able
to tune model parameters, so I am going to incorporate/add that flavor
into the model.

```{r}

kknn_recipe <-
  recipe(formula = sex ~ ., data = penguin_train) %>%
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors(), -all_nominal())

#added flavour
kknn_recipe %>% prep() %>% juice() %>% summary()

#added flavour
c_metrics <- yardstick::metric_set(accuracy,
                        sens, spec,
                        roc_auc, mn_log_loss)
#added flavour
model_control <- tune::control_grid(save_pred = TRUE)

#added flavour
cores <- parallel::detectCores()
cores

kknn_spec <-
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%
  set_mode("classification") %>%
  set_engine("kknn")

knn_grid <- dials::grid_regular(parameters(kknn_spec), levels = 5)


kknn_workflow <-
  workflow() %>%
  add_recipe(kknn_recipe) %>%
  add_model(kknn_spec)

set.seed(40477)
# kknn_tune <-tune_grid(kknn_workflow, resamples = , grid = )
# Changed flavour

# Lets create our own tune instead

doParallel::registerDoParallel()
knn_tune <- tune_grid(
  kknn_spec,
  kknn_recipe,
  resamples = k_folds,
  control = model_control,
  metrics = c_metrics
)
```

I wrote this as a function which returns all the model parameters called
`customised_model_yardstick`

```{r}
pesonalised_model<-customised_model_yardstick()

```

A lot of information is hold in the output of that function

```{r}
names(pesonalised_model)
```

For instance we can have a look at the specified recipe

```{r}
pesonalised_model$recipe
```

# Visualising model performance

We can visualise how the model performs for all the defined metric
functions

```{r}
knn_tune %>%
  tune::collect_metrics() %>%
  ggplot(aes(x = neighbors, y = mean)) +
  geom_point() +
  geom_line() +
  facet_wrap(~.metric, scales = "free_y")
```

As we can see, accuracy, roc_auc, sensitivity and specificity increase
with the number of neighbors. On the contrary, loss decreases with the
number of neighbors.

We can visualise the variance between each cross-validation folds
(partition dataset)

```{r}
knn_tune %>%
  select(id, .metrics) %>%
  unnest(.metrics) %>%
  ggplot(aes(x = neighbors, y = .estimate, color = id)) +
  geom_point() +
  geom_line() +
  facet_wrap(~.metric, scales = "free_y") +
  theme(legend.position = "none")
```

We see that there is not much stability for sensitivity and specificity.

# Confusion matrix

A confusion matrix is a summary of prediction results on a
classification problem. We can calculated with the yardstick function
`conf_mat`.

```{r}
c_data_metric <- knn_tune %>%
  collect_predictions() %>%
  mutate(pred = if_else(.pred_female >= .5,
                        "female", "male"),
         pred = as.factor(pred))
c_data_metric %>% 
  yardstick::conf_mat(sex, pred)

c_data_metric %>% 
  yardstick::conf_mat(sex, pred) ->cm
```

We can get all the estimates for our confusion metric

```{r}
c_data_metric %>% 
  yardstick::conf_mat(sex, pred)  %>%
    summary()
```

And we can have a tidy version of our confusion matrix

```{r}
broom::tidy(cm)
```

# Confussion matrix components

In the [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) page,
we can see that what each term measures.

1.  Accuracy

Accuracy is the fraction of predictions the model got right. misleading
results if the data set is unbalanced. c_data_metric %\>% accuracy(sex,
pred)

```{r}
c_data_metric %>% 
    accuracy(sex, pred)

(tidy(cm)$value[1]+tidy(cm)$value[4])/sum(tidy(cm)$value)

```

2.  Sensitivy or recall

Sensitivy looks at how sensitive models are to False Negatives

```{r}
c_data_metric %>%
    sens(sex, pred)

(tidy(cm)$value[1])/(tidy(cm)$value[1]+tidy(cm)$value[2])
```

3\. Precision

Precision measures how sensitive model is to False Positives

```{r}
c_data_metric %>%
    ppv(sex, pred)

(tidy(cm)$value[1])/(tidy(cm)$value[1]+tidy(cm)$value[3])

```

4\. F1 Score

F1 measures the harmonic average of the precision and recall.

```{r}
 c_data_metric %>%
     f_meas(sex,.pred_class)
```

We can plot the roc curves for the models

```{r}
knn_tune %>% collect_predictions() %>% 
    group_by(id) %>% 
    roc_curve(sex, .pred_female) %>%
    autoplot()
```

# Selecting best model

We can see what model have us the best roc_auc

```{r}
knn_tune %>% tune::select_best(metric="roc_auc")
```

14 neighbors has best roc auc score, so we can add that into the model

```{r}
knn_model <- nearest_neighbor(neighbors =14) %>% 
    set_mode("classification") %>% 
    set_engine("kknn")
```

We can actually add the final model into a workflow and add the recipe
into that workflow as well

```{r}
final_model <- workflow() %>%
    add_model(knn_model) %>% 
    add_recipe(kknn_recipe)
```

So after determining the best model, the final fit on the entire
training set is needed; we can do that with `last_fit`.

```{r}
final_res <- tune::last_fit(final_model, penguin_split)
```

We can see how our model will predict on our test unseen data.

```{r}
final_res %>%
    tune::collect_predictions() %>% 
    mutate(correct = case_when( sex == .pred_class ~ "correct", TRUE ~ "incorrect" )) %>%
    bind_cols(penguin_test) %>% 
    ggplot(aes(sex...14, color=correct)) + 
    geom_bar() + labs(color = NULL) + xlab("")
```
